{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('<path_to_file>/Blueberry_train_cubes.npy', allow_pickle= True)\n",
    "labels = np.load('<path_to_file>/Blueberry_train_cubes_labels.npy', allow_pickle= True)\n",
    "\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = data.mean(axis=(0, 1, 2), keepdims=True)\n",
    "std_dev = data.std(axis=(0, 1, 2), keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these 2 lines to save normalization parameters\n",
    "np.save('Blueberry_train_cubes_mean.npy', mean)\n",
    "np.save('Blueberry_train_cubes_std.npy', std_dev)  # <-- NEW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Z-score normalization\n",
    "data = (data - mean) / std_dev\n",
    "\n",
    "print(\"Dataset shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA START\n",
    "hsi_image_train = data\n",
    "\n",
    "# Reshape\n",
    "hsi_reshaped_train = hsi_image_train.reshape(-1, 462)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA()\n",
    "hsi_train_pca = pca.fit_transform(hsi_reshaped_train)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Dimensions')\n",
    "plt.ylabel('Total Variance Retained')\n",
    "plt.title('Variance Retained vs No. of Dimensions')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of Dimensions that retains 99% variance:\n",
    "optimal_components = np.argmax(cumulative_explained_variance >= 0.999) + 1 \n",
    "print(f\"Number of Dimensions to retain 99% variance: {optimal_components}\")\n",
    "\n",
    "# Apply PCA with optimal components\n",
    "pca = PCA(n_components=optimal_components)\n",
    "X_train_PCA = pca.fit_transform(hsi_reshaped_train)\n",
    "\n",
    "X_train_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA components and mean\n",
    "np.save('Blueberry_train_cubes_PCA_components.npy', pca.components_)\n",
    "np.save('Blueberry_train_cubes_PCA_mean.npy', pca.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape back to the spatial dimensions\n",
    "data_train = X_train_PCA.reshape(2938, 32, 32, optimal_components)\n",
    "\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PCA Training and Testing set\n",
    "np.save('Blueberry_train_cubes_PCA.npy', data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PCA DATA\n",
    "data_train = np.load('Blueberry_train_cubes_PCA.npy')\n",
    "train_labels = np.load('Blueberry_train_cubes_labels.npy')\n",
    "\n",
    "print(data_train.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLY PCA TO TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load('Blueberry_test_cubes.npy', allow_pickle=True)\n",
    "\n",
    "# Load training normalization parameters\n",
    "train_mean = np.load('Blueberry_train_cubes_mean.npy')\n",
    "train_std = np.load('Blueberry_train_cubes_std.npy')\n",
    "\n",
    "# Normalize test data using TRAINING'S mean and std\n",
    "test_data_normalized = (test_data - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape test data to 2D\n",
    "num_test_samples = test_data.shape[0]\n",
    "hsi_reshaped_test = test_data_normalized.reshape(num_test_samples * 32 * 32, 462)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCA parameters from training\n",
    "pca_components = np.load('Blueberry_train_cubes_PCA_components.npy')\n",
    "pca_mean = np.load('Blueberry_train_cubes_PCA_mean.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA transformation\n",
    "hsi_test_pca = (hsi_reshaped_test - pca_mean) @ pca_components.T\n",
    "\n",
    "# Reshape back to 4D (preserve spatial dimensions)\n",
    "optimal_components = pca_components.shape[0]  # Get from component shape\n",
    "test_data_pca = hsi_test_pca.reshape(num_test_samples, 32, 32, optimal_components)\n",
    "\n",
    "# Save transformed test data\n",
    "np.save('Blueberry_test_cubes_PCA.npy', test_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD PCA DATA\n",
    "train_labels = np.load('Blueberry_train_cubes_labels.npy')\n",
    "test_labels = np.load('Blueberry_test_cubes_labels.npy')\n",
    "\n",
    "print(train_labels.shape, test_labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique labels and counts\n",
    "# Corrected function with proper variable names\n",
    "def check_class_distribution(labels, dataset_name):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{dataset_name} Set Class Distribution:\")\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"Class {cls}: {count} samples ({count/len(labels)*100:.2f}%)\")\n",
    "    return unique, counts\n",
    "\n",
    "# For training set\n",
    "train_unique, train_counts = check_class_distribution(train_labels, \"Training\")\n",
    "test_unique, test_counts = check_class_distribution(test_labels, \"Testing\")\n",
    "\n",
    "# Rest of the analysis code remains the same...\n",
    "\n",
    "# Check if test classes match train classes\n",
    "if set(train_unique) != set(test_unique):\n",
    "    print(\"\\nWarning: Mismatched classes between train and test sets!\")\n",
    "    print(f\"Train classes: {train_unique}\")\n",
    "    print(f\"Test classes: {test_unique}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(train_unique, train_counts)\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(test_unique, test_counts)\n",
    "plt.title('Test Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "def calculate_imbalance_ratio(counts):\n",
    "    majority = max(counts)\n",
    "    minority = min(counts)\n",
    "    return majority/minority\n",
    "\n",
    "train_ratio = calculate_imbalance_ratio(train_counts)\n",
    "test_ratio = calculate_imbalance_ratio(test_counts)\n",
    "\n",
    "print(f\"\\nTraining set imbalance ratio: {train_ratio:.2f}:1\")\n",
    "print(f\"Test set imbalance ratio: {test_ratio:.2f}:1\")\n",
    "\n",
    "# Class imbalance thresholds\n",
    "if train_ratio > 4:\n",
    "    print(\"\\nSevere class imbalance detected in training data!\")\n",
    "elif train_ratio > 2:\n",
    "    print(\"\\nModerate class imbalance detected in training data.\")\n",
    "else:\n",
    "    print(\"\\nClasses are relatively balanced in training data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
