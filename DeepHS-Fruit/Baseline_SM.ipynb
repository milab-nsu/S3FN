{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('<path_to_file>/SM.npy', allow_pickle=True)\n",
    "\n",
    "# Check the shape of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_spectral_curves(data, num_samples):\n",
    "    \n",
    "    # Exclude the label column\n",
    "    spectral_data = data[:, :-1]\n",
    "    labels = data[:, -1]\n",
    "\n",
    "    # Randomly choose `num_samples` rows\n",
    "    random_indices = np.random.choice(spectral_data.shape[0], num_samples, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        \n",
    "        # Extract the label\n",
    "        label = labels[idx]\n",
    "\n",
    "        # Extract the spectral values\n",
    "        spectral_values = spectral_data[idx].astype(float)\n",
    "        wavelengths = np.arange(1, spectral_values.shape[0] + 1)\n",
    "\n",
    "        # Plot the spectral curve\n",
    "        plt.plot(wavelengths, spectral_values, label=f\"Sample {idx} - Label: {label}\")\n",
    "\n",
    "    plt.title(f\"Spectral Curves for {num_samples} Random Samples\")\n",
    "    plt.xlabel(\"Bands\")\n",
    "    plt.ylabel(\"Mean Reflectance\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_random_spectral_curves(data, num_samples = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= data[:, :-1]\n",
    "y= data[:, -1]  \n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "Xn = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xn, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train.shape,y_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder\n",
    "le= LabelEncoder()\n",
    "y_encoded= le.fit_transform(y)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(Xn, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train2.shape,y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.unique(y)\n",
    "print(unique_classes)\n",
    "\n",
    "unique_classes_encoded = np.unique(y_encoded)\n",
    "print(unique_classes_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAPER HYPERPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree with 'gini' criterion\n",
    "dt_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "dt_gini.fit(X_train, y_train)\n",
    "\n",
    "accuracy_gini = accuracy_score(y_test, dt_gini.predict(X_test))\n",
    "\n",
    "print(\"Decision Tree with Gini Accuracy:\", accuracy_gini * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with 'entropy' criterion\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "\n",
    "accuracy_entropy = accuracy_score(y_test, dt_entropy.predict(X_test))\n",
    "print(\"Decision Tree with Entropy Accuracy:\", accuracy_entropy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest: 100 trees, 'log_loss' criterion\n",
    "rf_log_loss = RandomForestClassifier(n_estimators=100, criterion='log_loss', random_state=42)\n",
    "rf_log_loss.fit(X_train, y_train)\n",
    "\n",
    "accuracy_rf_log_loss = accuracy_score(y_test, rf_log_loss.predict(X_test))\n",
    "\n",
    "print(\"Random Forest (log_loss, 100 trees) Accuracy:\", accuracy_rf_log_loss * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: 50 trees, 'entropy' criterion\n",
    "rf_50_entropy = RandomForestClassifier(n_estimators=50, criterion='entropy', random_state=42)\n",
    "rf_50_entropy.fit(X_train, y_train)\n",
    "\n",
    "accuracy_rf_50_entropy = accuracy_score(y_test, rf_50_entropy.predict(X_test))\n",
    "\n",
    "print(\"Random Forest (50 trees, entropy) Accuracy:\", accuracy_rf_50_entropy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: 90 trees, 'entropy' criterion\n",
    "rf_90_entropy = RandomForestClassifier(n_estimators=90, criterion='entropy', random_state=42)\n",
    "rf_90_entropy.fit(X_train, y_train)\n",
    "\n",
    "accuracy_rf_90_entropy = accuracy_score(y_test, rf_90_entropy.predict(X_test))\n",
    "\n",
    "print(\"Random Forest (90 trees, entropy) Accuracy:\", accuracy_rf_90_entropy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression: No penalty, solver 'newton-cg'\n",
    "lr_no_penalty = LogisticRegression(penalty=None, solver='newton-cg', random_state=42)\n",
    "lr_no_penalty.fit(X_train, y_train)\n",
    "\n",
    "accuracy_lr_no_penalty = accuracy_score(y_test, lr_no_penalty.predict(X_test))\n",
    "\n",
    "print(\"Logistic Regression (No penalty) Accuracy:\", accuracy_lr_no_penalty * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: L2 penalty, solver 'lbfgs'\n",
    "lr_l2_lbfgs = LogisticRegression(penalty='l2', solver='lbfgs', random_state=42, max_iter=1000)\n",
    "lr_l2_lbfgs.fit(X_train, y_train)\n",
    "\n",
    "accuracy_lr_l2_lbfgs = accuracy_score(y_test, lr_l2_lbfgs.predict(X_test))\n",
    "print(\"Logistic Regression (L2 penalty, lbfgs) Accuracy:\", accuracy_lr_l2_lbfgs * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting: 150 stages\n",
    "gb_150 = GradientBoostingClassifier(n_estimators=150, random_state=42)\n",
    "gb_150.fit(X_train, y_train)\n",
    "accuracy_gb_150 = accuracy_score(y_test, gb_150.predict(X_test))\n",
    "print(\"Gradient Boosting (150 stages) Accuracy:\", accuracy_gb_150 * 100)\n",
    "\n",
    "# Gradient Boosting: 450 stages\n",
    "gb_450 = GradientBoostingClassifier(n_estimators=450, random_state=42)\n",
    "gb_450.fit(X_train, y_train)\n",
    "accuracy_gb_450 = accuracy_score(y_test, gb_450.predict(X_test))\n",
    "print(\"Gradient Boosting (450 stages) Accuracy:\", accuracy_gb_450 * 100)\n",
    "\n",
    "# Gradient Boosting: 250 stages\n",
    "gb_250 = GradientBoostingClassifier(n_estimators=250, random_state=42)\n",
    "gb_250.fit(X_train, y_train)\n",
    "accuracy_gb_250 = accuracy_score(y_test, gb_250.predict(X_test))\n",
    "print(\"Gradient Boosting (250 stages) Accuracy:\", accuracy_gb_250 * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR HYPERPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \n",
    "    'n_neighbors': [3, 5],  # Use fewer values to test\n",
    "    'weights': ['uniform'],  # Try with just one weight for now\n",
    "    'metric': ['euclidean']  # Test one distance metric at a time\n",
    "}\n",
    "\n",
    "# KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_knn = GridSearchCV(estimator=knn_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Training\n",
    "grid_search_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Accuracy\n",
    "y_test_pred = grid_search_knn.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(y_test_pred == y_test) * 100\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [7],\n",
    "    'learning_rate': [0.01],\n",
    "    'n_estimators': [100]\n",
    "}\n",
    "\n",
    "\n",
    "# GridSearchCV for XGBoost Classifier\n",
    "xgb_grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "xgb_grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "# Get best parameters for XGBoost Classifier\n",
    "best_params = xgb_grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator\n",
    "best_xgb_model = xgb_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_xgb_model.predict(X_test2)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test2, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "param_dist = {\n",
    "    \n",
    "    'C': [0.1, 1, 10, 100],         \n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  \n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4, 5], \n",
    "    'coef0': [0.0, 0.1, 0.5, 1.0] \n",
    "    \n",
    "}\n",
    "# SVM classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Instantiate the grid search\n",
    "random_search = RandomizedSearchCV(svc, param_distributions=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Training\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters found by RandomizedSearchCV\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "# Use the best model to predict on the test data\n",
    "y_pred = random_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Accuracy\n",
    "y_test_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(y_test_pred == y_test) * 100\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto(gpu_options =\n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    ")\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network\n",
    "\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(462,)),\n",
    "        Dense(units=30, activation='relu', kernel_regularizer=regularizers.l2(0.001), name='layer1'),\n",
    "        #Dense(units=15, activation='relu', kernel_regularizer=regularizers.l2(0.001), name='layer2'),\n",
    "        #Dense(units=8, activation='relu', kernel_regularizer=regularizers.l2(0.0001), name='layer3'),\n",
    "        \n",
    "        Dense(units=2, activation='linear')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "model.compile(\n",
    "    \n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    \n",
    "    X_train2,y_train2,\n",
    "    batch_size = 500,\n",
    "    epochs = 1000,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor = 'loss',\n",
    "            patience = 500,\n",
    "            restore_best_weights = True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test2)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "#Accuracy\n",
    "# Calculate total accuracy\n",
    "total_accuracy = accuracy_score(y_test2, predicted_labels)\n",
    "\n",
    "print(f\"Total Test Accuracy: {total_accuracy*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
