{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVI_TO_NP = {\n",
    "    1: np.uint8,\n",
    "    2: np.int16,\n",
    "    3: np.int32,\n",
    "    4: np.float32,\n",
    "    5: np.float64,\n",
    "    12: np.uint16,\n",
    "    13: np.uint32,\n",
    "    14: np.int64,\n",
    "    15: np.uint64\n",
    "}\n",
    "\n",
    "# Cell 2: Utility Functions\n",
    "def parse_envi_header(hdr_path):\n",
    "    \"\"\"Parse ENVI header file into a dictionary\"\"\"\n",
    "    header = {}\n",
    "    with open(hdr_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                key = key.strip().lower()\n",
    "                value = value.strip()\n",
    "                header[key] = value\n",
    "    return header\n",
    "\n",
    "def get_file_list(input_root):\n",
    "    \"\"\"Find all .bin files in directory structure\"\"\"\n",
    "    bin_files = []\n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.bin'):\n",
    "                bin_files.append(os.path.join(root, file))\n",
    "    return bin_files\n",
    "\n",
    "# Cell 3: Processing Functions\n",
    "def process_hsi_file(bin_path, output_root):\n",
    "    \"\"\"Process a single HSI file and return statistics\"\"\"\n",
    "    hdr_path = os.path.splitext(bin_path)[0] + '.hdr'\n",
    "    if not os.path.exists(hdr_path):\n",
    "        raise FileNotFoundError(f\"No header file found for {bin_path}\")\n",
    "\n",
    "    header = parse_envi_header(hdr_path)\n",
    "    \n",
    "    height = int(header['lines'])\n",
    "    width = int(header['samples'])\n",
    "    bands = int(header['bands'])\n",
    "    dtype = ENVI_TO_NP[int(header['data type'])]\n",
    "    interleave = header.get('interleave', 'bil').lower()\n",
    "    byte_order = '<' if int(header.get('byte order', 0)) == 0 else '>'\n",
    "    dtype = np.dtype(dtype).newbyteorder(byte_order)\n",
    "\n",
    "    with open(bin_path, 'rb') as f:\n",
    "        data = np.fromfile(f, dtype=dtype)\n",
    "    \n",
    "    if interleave == 'bil':\n",
    "        data = data.reshape((height, bands, width)).transpose(0, 2, 1)\n",
    "    elif interleave == 'bip':\n",
    "        data = data.reshape((height, width, bands))\n",
    "    elif interleave == 'bsq':\n",
    "        data = data.reshape((bands, height, width)).transpose(1, 2, 0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported interleave format: {interleave}\")\n",
    "\n",
    "    # Save to npy\n",
    "    rel_path = os.path.splitext(os.path.relpath(bin_path, input_root))[0]\n",
    "    output_path = os.path.join(output_root, rel_path + '.npy')\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    np.save(output_path, data)\n",
    "\n",
    "    return {\n",
    "        'height': height,\n",
    "        'width': width,\n",
    "        'bands': bands,\n",
    "        'pixel_min': np.min(data),\n",
    "        'pixel_max': np.max(data),\n",
    "        'pixel_sum': np.sum(data),\n",
    "        'pixel_count': data.size\n",
    "    }\n",
    "\n",
    "# Cell 4: Main Processing and Statistics\n",
    "def process_all_files(input_root, output_root):\n",
    "    \"\"\"Process all files and collect statistics\"\"\"\n",
    "    file_list = get_file_list(input_root)\n",
    "    all_stats = []\n",
    "    global_stats = {\n",
    "        'total_pixel_sum': 0,\n",
    "        'total_pixel_count': 0,\n",
    "        'global_pixel_min': None,\n",
    "        'global_pixel_max': None,\n",
    "        'dim_stats': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    for i, bin_path in enumerate(file_list):\n",
    "        try:\n",
    "            print(f\"Processing {i+1}/{len(file_list)}: {bin_path}\")\n",
    "            stats = process_hsi_file(bin_path, output_root)\n",
    "            all_stats.append(stats)\n",
    "            \n",
    "            # Update global statistics\n",
    "            global_stats['total_pixel_sum'] += stats['pixel_sum']\n",
    "            global_stats['total_pixel_count'] += stats['pixel_count']\n",
    "            \n",
    "            if (global_stats['global_pixel_min'] is None or \n",
    "                stats['pixel_min'] < global_stats['global_pixel_min']):\n",
    "                global_stats['global_pixel_min'] = stats['pixel_min']\n",
    "            \n",
    "            if (global_stats['global_pixel_max'] is None or \n",
    "                stats['pixel_max'] > global_stats['global_pixel_max']):\n",
    "                global_stats['global_pixel_max'] = stats['pixel_max']\n",
    "            \n",
    "            # Update dimension stats\n",
    "            for dim in ['height', 'width', 'bands']:\n",
    "                global_stats['dim_stats'][dim].append(stats[dim])\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {bin_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return global_stats, all_stats\n",
    "\n",
    "# Cell 5: Visualization and Reporting\n",
    "def print_statistics(global_stats):\n",
    "    \"\"\"Print formatted statistics\"\"\"\n",
    "    print(\"Dimension Statistics:\")\n",
    "    for dim in ['height', 'width', 'bands']:\n",
    "        values = global_stats['dim_stats'][dim]\n",
    "        print(f\"{dim.capitalize()}:\")\n",
    "        print(f\"  Min: {min(values)}\")\n",
    "        print(f\"  Max: {max(values)}\")\n",
    "        print(f\"  Avg: {sum(values)/len(values):.2f}\")\n",
    "    \n",
    "    print(\"\\nPixel Value Statistics:\")\n",
    "    print(f\"Global Minimum: {global_stats['global_pixel_min']}\")\n",
    "    print(f\"Global Maximum: {global_stats['global_pixel_max']}\")\n",
    "    if global_stats['total_pixel_count'] > 0:\n",
    "        print(f\"Global Mean: {global_stats['total_pixel_sum'] / global_stats['total_pixel_count']:.4f}\")\n",
    "\n",
    "def plot_dimension_distributions(global_stats):\n",
    "    \"\"\"Plot histograms for dimensions\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    for i, dim in enumerate(['height', 'width', 'bands']):\n",
    "        axs[i].hist(global_stats['dim_stats'][dim], bins=20, alpha=0.7)\n",
    "        axs[i].set_title(f'{dim.capitalize()} Distribution')\n",
    "        axs[i].set_xlabel(dim)\n",
    "        axs[i].set_ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Execution Cell (Edit paths here before running)\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set your paths here\n",
    "    input_root = r'<path_to_input_root>/Ripeness'\n",
    "    output_root = r'<path_to_output_root>/Ripeness_npy'\n",
    "    \n",
    "    # Process files and get statistics\n",
    "    global_stats, all_stats = process_all_files(input_root, output_root)\n",
    "    \n",
    "    # Print statistics\n",
    "    print_statistics(global_stats)\n",
    "    \n",
    "    # Show visualizations\n",
    "    plot_dimension_distributions(global_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(values):\n",
    "    \n",
    "    \"\"\"Compute min, max, and average of a list.\"\"\"\n",
    "    if not values:\n",
    "        return None, None, None\n",
    "    return min(values), max(values), sum(values) / len(values)\n",
    "\n",
    "def analyze_hsi_dataset(root_dir):\n",
    "    \n",
    "    # Initialize lists to store dimensions\n",
    "    heights = []\n",
    "    widths = []\n",
    "    bands_list = []\n",
    "    \n",
    "    # Traverse directory structure\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        \n",
    "        for file in files:\n",
    "            \n",
    "            if file.endswith('.npy'):\n",
    "                \n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    arr = np.load(file_path)\n",
    "                    \n",
    "                    if arr.ndim != 3:\n",
    "                        print(f\"Skipping {file_path}: Expected 3D array, got {arr.ndim}D\")\n",
    "                        \n",
    "                        continue\n",
    "                    \n",
    "                    # Assuming shape is (height, width, bands)\n",
    "                    h, w, b = arr.shape  # Adjust indices if your data uses a different order\n",
    "                    \n",
    "                    heights.append(h)\n",
    "                    widths.append(w)\n",
    "                    bands_list.append(b)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Compute statistics\n",
    "    h_min, h_max, h_avg = compute_stats(heights)\n",
    "    w_min, w_max, w_avg = compute_stats(widths)\n",
    "    b_min, b_max, b_avg = compute_stats(bands_list)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Height Statistics:\")\n",
    "    print(f\"  Min: {h_min}, Max: {h_max}, Average: {h_avg:.2f}\")\n",
    "    print(\"\\nWidth Statistics:\")\n",
    "    print(f\"  Min: {w_min}, Max: {w_max}, Average: {w_avg:.2f}\")\n",
    "    print(\"\\nBands Statistics:\")\n",
    "    print(f\"  Min: {b_min}, Max: {b_max}, Average: {b_avg:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    root_directory = r'<path_to_directory>/VIS'\n",
    "    analyze_hsi_dataset(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_hsi(arr, target_h, target_w, preserve_range=True):\n",
    "    \n",
    "    # Initialize resized array\n",
    "    resized = np.zeros((target_h, target_w, arr.shape[2]), dtype=arr.dtype)\n",
    "    \n",
    "    # Resize each band individually\n",
    "    for band in range(arr.shape[2]):\n",
    "        resized_band = resize(\n",
    "            arr[:, :, band],\n",
    "            (target_h, target_w),\n",
    "            preserve_range=preserve_range,\n",
    "            anti_aliasing=True  # Reduces artifacts\n",
    "        )\n",
    "        resized[:, :, band] = resized_band\n",
    "        \n",
    "    return resized\n",
    "\n",
    "\n",
    "def process_hsi_dataset(input_root, output_root, target_h, target_w):\n",
    "    \n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                # Create output directory structure\n",
    "                rel_path = os.path.relpath(root, input_root)\n",
    "                output_dir = os.path.join(output_root, rel_path)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                \n",
    "                # Process file\n",
    "                input_path = os.path.join(root, file)\n",
    "                output_path = os.path.join(output_dir, file)\n",
    "                \n",
    "                try:\n",
    "                    # Load original HSI\n",
    "                    arr = np.load(input_path)\n",
    "                    \n",
    "                    if arr.ndim != 3:\n",
    "                        print(f\"Skipping {input_path}: Not a 3D array\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Resize and save\n",
    "                    resized_arr = resize_hsi(arr, target_h, target_w)\n",
    "                    np.save(output_path, resized_arr)\n",
    "                    print(f\"Processed: {input_path} -> {output_path}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    input_directory = r'<path_to_input_directory>/VIS'\n",
    "    output_directory = r'<path_to_output_directory>/VIS_resized'\n",
    "\n",
    "    target_height = 250  \n",
    "    target_width = 200   \n",
    "    \n",
    "    # Run processing\n",
    "    process_hsi_dataset(\n",
    "        input_root=input_directory,\n",
    "        output_root=output_directory,\n",
    "        target_h=target_height,\n",
    "        target_w=target_width\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_hsi(directory, num_images=4, figsize=(15, 10)):\n",
    "    # Find all .npy files in directory\n",
    "    hsi_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                hsi_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if not hsi_files:\n",
    "        print(\"No .npy files found in directory!\")\n",
    "        return\n",
    "\n",
    "    # Select random files\n",
    "    selected_files = random.sample(hsi_files, min(num_images, len(hsi_files)))\n",
    "    \n",
    "    # Create subplots\n",
    "    rows = int(np.sqrt(num_images))\n",
    "    cols = int(np.ceil(num_images / rows))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.ravel() if num_images > 1 else [axes]\n",
    "\n",
    "    for idx, file_path in enumerate(selected_files):\n",
    "        try:\n",
    "            # Load HSI cube\n",
    "            hsi = np.load(file_path)\n",
    "            \n",
    "            if hsi.ndim != 3:\n",
    "                print(f\"Skipping {file_path}: Not a 3D array\")\n",
    "                continue\n",
    "\n",
    "            # Create pseudo-color image (mean across bands)\n",
    "            img = np.mean(hsi, axis=2)  # Change axis if different dimension order\n",
    "            \n",
    "            # Plot with Viridis colormap\n",
    "            axes[idx].imshow(img, cmap='viridis')\n",
    "            axes[idx].set_title(os.path.basename(file_path))\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Hide empty axes\n",
    "    for j in range(idx+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_directory = r'<path_to_output_directory>/VIS_resized'\n",
    "\n",
    "    \n",
    "    visualize_random_hsi(\n",
    "        \n",
    "        directory=output_directory,\n",
    "        num_images=4,        # Number of images to display\n",
    "        figsize=(15, 10)     # Figure size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(source_dir, output_dir, train_ratio=0.75, val_ratio=0.125, test_ratio=0.125):\n",
    "    \n",
    "    if not os.path.exists(source_dir):\n",
    "        \n",
    "        print(f\"Source directory '{source_dir}' does not exist.\")\n",
    "        \n",
    "        return\n",
    "\n",
    "    # Ensure ratios sum up to 1\n",
    "    assert train_ratio + val_ratio + test_ratio == 1, \"Ratios must sum up to 1\"\n",
    "\n",
    "    # Define output subdirectories\n",
    "    train_dir = os.path.join(output_dir, \"train\")\n",
    "    val_dir = os.path.join(output_dir, \"val\")\n",
    "    test_dir = os.path.join(output_dir, \"test\")\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    for folder in [train_dir, val_dir, test_dir]:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Walk through dataset and split files\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if not files:\n",
    "            continue  # Skip empty directories\n",
    "        \n",
    "        # Relative path from source directory\n",
    "        rel_path = os.path.relpath(root, source_dir)\n",
    "\n",
    "        # Create corresponding folders in train, val, test\n",
    "        for subset_dir in [train_dir, val_dir, test_dir]:\n",
    "            os.makedirs(os.path.join(subset_dir, rel_path), exist_ok=True)\n",
    "\n",
    "        # Shuffle and split files\n",
    "        random.shuffle(files)\n",
    "        total_files = len(files)\n",
    "        train_split = int(total_files * train_ratio)\n",
    "        val_split = int(total_files * val_ratio)\n",
    "\n",
    "        train_files = files[:train_split]\n",
    "        val_files = files[train_split:train_split + val_split]\n",
    "        test_files = files[train_split + val_split:]\n",
    "\n",
    "        # Copy files to respective directories\n",
    "        for file in train_files:\n",
    "            shutil.copy2(os.path.join(root, file), os.path.join(train_dir, rel_path, file))\n",
    "        for file in val_files:\n",
    "            shutil.copy2(os.path.join(root, file), os.path.join(val_dir, rel_path, file))\n",
    "        for file in test_files:\n",
    "            shutil.copy2(os.path.join(root, file), os.path.join(test_dir, rel_path, file))\n",
    "\n",
    "    print(f\"Dataset split complete. Check '{output_dir}' for the split dataset.\")\n",
    "\n",
    "source_dataset = r'<path_to_source_dataset>/NIR_resized'\n",
    "split_output = r'<path_to_split_output>/NIR_resized_Split'\n",
    "\n",
    "split_dataset(source_dataset, split_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
